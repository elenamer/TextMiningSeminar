{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import collections\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook BERT Kaggle Notebook.ipynb to script\n",
      "[NbConvertApp] Writing 5506 bytes to BERT Kaggle Notebook.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script \"BERT Kaggle Notebook.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = .15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "# read full dataset file\n",
    "with open(\"/kaggle/input/10kgnad/articles.csv\",  \"r\", encoding='utf-8',) as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=';', quotechar='\\'')\n",
    "    for row in reader:\n",
    "        labels.append(row[0])\n",
    "        texts.append(row[1])\n",
    "\n",
    "tr_Df=pd.concat([pd.Series(texts, name=\"Text\"), pd.Series(labels,name='Label')], axis=1)\n",
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split( tr_Df['Text'], tr_Df['Label'], test_size=SPLIT, random_state=42,stratify=tr_Df['Label'])\n",
    "\n",
    "# write train and test datasets\n",
    "train=pd.DataFrame(pd.concat([X_train,y_train],axis=1))\n",
    "test=pd.DataFrame(pd.concat([X_test,y_test],axis=1))\n",
    "\n",
    "# ## BERT\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "sms = train.Text.values\n",
    "[print(len(sm.split(\" \"))) for sm in sms]\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def gen_id_mask(data):\n",
    "    sms = data.Text.values\n",
    "    sms = [\"[CLS] \" + sm + \" [SEP]\" for sm in sms]\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
    "\n",
    "    tokenized_texts = [tokenizer.tokenize(sm) for sm in sms]\n",
    "\n",
    "    MAX_LEN = 512\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    attention_masks = []\n",
    "\n",
    "    # Create a mask of 1s for each token followed by 0s for padding\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    print('success')\n",
    "    return (input_ids,attention_masks)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "train_input_ids,train_att_masks=gen_id_mask(train)\n",
    "test_input_ids, test_att_masks=gen_id_mask(test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9321</th>\n",
       "      <td>Die Energiewirtschaft hat ihre Strategie bis 2...</td>\n",
       "      <td>Wirtschaft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5703</th>\n",
       "      <td>Östereich siegte in Podgorica zuerst gegen 12,...</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8630</th>\n",
       "      <td>Sagis Vertreter Phillip Burns und Barry Gilber...</td>\n",
       "      <td>Wirtschaft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Keine offizielle Bestätigung über Verhandlungs...</td>\n",
       "      <td>Etat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>Roland Düringer in Autorevue TV, vom Leiden de...</td>\n",
       "      <td>Etat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>Auch Einreise- und Vermögenssperren gegen Luka...</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236</th>\n",
       "      <td>In der chinesischen Hauptstadt fahren Österrei...</td>\n",
       "      <td>Panorama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Strache will weltoffen sein, atmete man in dem...</td>\n",
       "      <td>Etat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5186</th>\n",
       "      <td>25-Jähriger wollte TV-Sender mit Sprengstoffgü...</td>\n",
       "      <td>Panorama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6466</th>\n",
       "      <td>Linz übernimmt nach Sieg in Innsbruck die Spit...</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3081 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text          Label\n",
       "9321  Die Energiewirtschaft hat ihre Strategie bis 2...     Wirtschaft\n",
       "5703  Östereich siegte in Podgorica zuerst gegen 12,...          Sport\n",
       "8630  Sagis Vertreter Phillip Burns und Barry Gilber...     Wirtschaft\n",
       "44    Keine offizielle Bestätigung über Verhandlungs...           Etat\n",
       "537   Roland Düringer in Autorevue TV, vom Leiden de...           Etat\n",
       "...                                                 ...            ...\n",
       "2184  Auch Einreise- und Vermögenssperren gegen Luka...  International\n",
       "5236  In der chinesischen Hauptstadt fahren Österrei...       Panorama\n",
       "220   Strache will weltoffen sein, atmete man in dem...           Etat\n",
       "5186  25-Jähriger wollte TV-Sender mit Sprengstoffgü...       Panorama\n",
       "6466  Linz übernimmt nach Sieg in Innsbruck die Spit...          Sport\n",
       "\n",
       "[3081 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = BertModel.from_pretrained('dbmdz/bert-base-german-cased', output_hidden_states=True)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "train_sentences_embeddings=[]\n",
    "test_sentences_embeddings=[]\n",
    "model = model.eval()\n",
    "\n",
    "for i in range(test_input_ids.shape[0]):\n",
    "    test_inputs = torch.tensor(test_input_ids[i:i+1])\n",
    "    #train_labels = torch.tensor(train_labels)\n",
    "    test_masks = torch.tensor(test_att_masks[i:i+1])\n",
    "    with torch.no_grad():\n",
    "        encoded_layers = model(input_ids=test_inputs, attention_mask=test_masks)\n",
    "\n",
    "    hidden_states = encoded_layers[2]\n",
    "\n",
    "    token_vecs = hidden_states[11][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    print(sentence_embedding.size())\n",
    "    test_sentences_embeddings.append(sentence_embedding.numpy())\n",
    "\n",
    "\n",
    "print(\"Test Done\")\n",
    "\n",
    "for i in range(train_input_ids.shape[0]):\n",
    "    train_inputs = torch.tensor(train_input_ids[i:i+1])\n",
    "    #train_labels = torch.tensor(train_labels)\n",
    "    train_masks = torch.tensor(train_att_masks[i:i+1])\n",
    "    with torch.no_grad():\n",
    "        encoded_layers = model(input_ids=train_inputs, attention_mask=train_masks)\n",
    "\n",
    "    hidden_states = encoded_layers[2]\n",
    "    token_vecs = hidden_states[11][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    #print(sentence_embedding.size())\n",
    "    train_sentences_embeddings.append(sentence_embedding.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_embeddings = np.stack(train_sentences_embeddings, axis=0)\n",
    "train_sentences_embeddings.shape\n",
    "test_sentences_embeddings = np.stack(test_sentences_embeddings, axis=0)\n",
    "test_sentences_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"BERT_last_hidden_train_sentences_embeddings.out\", train_sentences_embeddings, delimiter=\",\")\n",
    "#y_train.to_csv(\"y_train_labels.out\")\n",
    "#np.savetxt(\"BERT_last_hidden_test_sentences_embeddings.out\", test_sentences_embeddings, delimiter=\",\")\n",
    "#y_test.to_csv(\"y_test_labels.out\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=pd.read_csv(\"y_train_labels.out\", index_col=0, header=None)\n",
    "y_train=y_train.iloc[:,0]\n",
    "y_test=pd.read_csv(\"y_test_labels.out\", index_col=0, header=None) \n",
    "y_test=y_test.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_embeddings = np.genfromtxt('BERT_last_hidden_train_sentences_embeddings.out', delimiter=',')\n",
    "test_sentences_embeddings = np.genfromtxt('BERT_last_hidden_test_sentences_embeddings.out', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9989695154391335"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "clf = RandomForestClassifier(max_depth=15)\n",
    "clf.fit(train_sentences_embeddings,y_train)\n",
    "f1_score(clf.predict(train_sentences_embeddings),y_train, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Panorama         252\n",
       "Web              251\n",
       "International    227\n",
       "Wirtschaft       212\n",
       "Sport            180\n",
       "Inland           152\n",
       "Etat             100\n",
       "Wissenschaft      86\n",
       "Kultur            81\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.score(test_sentences_embeddings,[\"Panorama\"]*y_test.shape[0]))\n",
    "f1_score(clf.predict(test_sentences_embeddings),[\"Panorama\"]*y_test.shape[0], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8170019467878001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8174209868081426"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clf.score(test_sentences_embeddings,y_test))\n",
    "f1_score(clf.predict(test_sentences_embeddings),y_test, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d3022d7d782a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embds' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression(C=1000)\n",
    "clf.fit(train_sentences_embeddings,y_train)\n",
    "f1_score(clf.predict(train_sentences_embeddings),y_train, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(clf.predict(test_sentences_embeddings),y_test, average='weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
