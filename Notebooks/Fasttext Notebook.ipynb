{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import collections\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []\n",
    "\n",
    "# read full dataset file\n",
    "with open(\"../articles.csv\",  \"r\", encoding='utf-8',) as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=';', quotechar='\\'')\n",
    "    for row in reader:\n",
    "        labels.append(row[0])\n",
    "        texts.append(row[1])\n",
    "\n",
    "tr_Df=pd.concat([pd.Series(texts, name=\"Text\"), pd.Series(labels,name='Label')], axis=1)\n",
    "\n",
    "features_path=\"features\"\n",
    "if not os.path.exists(features_path):\n",
    "    os.mkdir(features_path)\n",
    "tr_Df.Label.to_csv(features_path+os.sep+\"y_labels.out\")\n",
    "\n",
    "# split dataset\n",
    "#X_train, X_test, y_train, y_test = train_test_split( tr_Df['Text'], tr_Df['Label'], test_size=SPLIT, random_state=42,stratify=tr_Df['Label'])\n",
    "\n",
    "# write train and test datasets\n",
    "#train=pd.DataFrame(pd.concat([X_train,y_train],axis=1))\n",
    "#test=pd.DataFrame(pd.concat([X_test,y_test],axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9321</th>\n",
       "      <td>Die Energiewirtschaft hat ihre Strategie bis 2...</td>\n",
       "      <td>Wirtschaft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5703</th>\n",
       "      <td>Östereich siegte in Podgorica zuerst gegen 12,...</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8630</th>\n",
       "      <td>Sagis Vertreter Phillip Burns und Barry Gilber...</td>\n",
       "      <td>Wirtschaft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Keine offizielle Bestätigung über Verhandlungs...</td>\n",
       "      <td>Etat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>Roland Düringer in Autorevue TV, vom Leiden de...</td>\n",
       "      <td>Etat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>Auch Einreise- und Vermögenssperren gegen Luka...</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236</th>\n",
       "      <td>In der chinesischen Hauptstadt fahren Österrei...</td>\n",
       "      <td>Panorama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Strache will weltoffen sein, atmete man in dem...</td>\n",
       "      <td>Etat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5186</th>\n",
       "      <td>25-Jähriger wollte TV-Sender mit Sprengstoffgü...</td>\n",
       "      <td>Panorama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6466</th>\n",
       "      <td>Linz übernimmt nach Sieg in Innsbruck die Spit...</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3081 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text          Label\n",
       "9321  Die Energiewirtschaft hat ihre Strategie bis 2...     Wirtschaft\n",
       "5703  Östereich siegte in Podgorica zuerst gegen 12,...          Sport\n",
       "8630  Sagis Vertreter Phillip Burns und Barry Gilber...     Wirtschaft\n",
       "44    Keine offizielle Bestätigung über Verhandlungs...           Etat\n",
       "537   Roland Düringer in Autorevue TV, vom Leiden de...           Etat\n",
       "...                                                 ...            ...\n",
       "2184  Auch Einreise- und Vermögenssperren gegen Luka...  International\n",
       "5236  In der chinesischen Hauptstadt fahren Österrei...       Panorama\n",
       "220   Strache will weltoffen sein, atmete man in dem...           Etat\n",
       "5186  25-Jähriger wollte TV-Sender mit Sprengstoffgü...       Panorama\n",
       "6466  Linz übernimmt nach Sieg in Innsbruck die Spit...          Sport\n",
       "\n",
       "[3081 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_multiple_whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download fasttext model \n",
    "\n",
    "From: https://fasttext.cc/docs/en/aligned-vectors.html\n",
    "\n",
    "Download German aligned vectors (filename: wiki.de.align.vec)\n",
    "in 'Notebooks/models/' folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = KeyedVectors.load_word2vec_format('./models/wiki.de.align.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"der\",\"die\",\"das\",\"den\",\"des\",\"dem\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Tokenizes the provided text\n",
    "    Args:\n",
    "        text (str): The text to be tokenized\n",
    "    Returns:\n",
    "        list(tuple(str, int)): A list of (token, count) pairs from the text without the stopwords.\n",
    "    \"\"\"\n",
    "\n",
    "    # make everything lowercase and strip punctuation\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_punctuation, strip_multiple_whitespaces]\n",
    "    tokens = preprocess_string(text, CUSTOM_FILTERS)\n",
    "\n",
    "    # filter out all stopwords\n",
    "    filtered_tokens = [w for w in tokens if not w in stopwords]\n",
    "\n",
    "    # return the filtered tokens\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(text):\n",
    "    \"\"\"Create the text embedding\n",
    "    Args:\n",
    "        text (str): The text to be embedded\n",
    "    Returns:\n",
    "        list(float): The array of values representing the text embedding\n",
    "    \"\"\"\n",
    "\n",
    "    # prepare the embedding placeholder\n",
    "    embedding = np.zeros(embeddings.vector_size, dtype=np.float32)\n",
    "\n",
    "    if text is None:\n",
    "        # return the default embedding in a vanilla python object\n",
    "        return embedding\n",
    "\n",
    "    # get the text terms with frequencies\n",
    "    tokens = tokenize(text)\n",
    "    # iterate through the terms and count the number of terms\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        # sum all token embeddings of the vector\n",
    "        if token in embeddings.vocab.keys():\n",
    "            embedding += embeddings[token]\n",
    "            count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        # return the empty embedding list\n",
    "        return embedding.tolist()\n",
    "\n",
    "    # average the embedding\n",
    "    embedding = embedding / count\n",
    "\n",
    "\n",
    "    # return the embedding in vanilla python object\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Fasttext embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_embeddings=[]\n",
    "for ind in tr_Df.index.values:\n",
    "    sentences_embeddings.append(text_embedding(tr_Df.Text.loc[ind]))\n",
    "\n",
    "sentences_embeddings=np.stack( sentences_embeddings, axis=0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(features_path):\n",
    "    os.mkdir(features_path)\n",
    "np.savetxt(features_path+os.sep+\"Fasttext_embeddings.out\", sentences_embeddings, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels=tr_Df.Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using FastText features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = .1\n",
    "N_FOLDS=10\n",
    "path=\"results_with_scaling_default\"\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "y_labels=pd.read_csv(features_path+os.sep+\"y_labels.out\", index_col=0, header=None) \n",
    "settings=\"fasttext\"\n",
    "sentences_embeddings = np.genfromtxt(features_path+os.sep+\"Fasttext_embeddings.out\", delimiter=',')\n",
    "kf=StratifiedKFold(n_splits=N_FOLDS)\n",
    "\n",
    "labels=y_labels.iloc[:,0].unique()\n",
    "for c in [\"knn\", \"gsvm\"]:#,\"logreg\"]:\n",
    "    kf=StratifiedKFold(n_splits=N_FOLDS)\n",
    "    conf_mat=np.zeros((9,9))\n",
    "    f1s=0\n",
    "    accs=0\n",
    "    scaler = MinMaxScaler()\n",
    "    for tr_ind, te_ind in kf.split(sentences_embeddings, y_labels):\n",
    "        X_tr_fold=sentences_embeddings[tr_ind]\n",
    "        X_te_fold=sentences_embeddings[te_ind]\n",
    "        y_tr_fold=y_labels.values[tr_ind]\n",
    "        y_te_fold=y_labels.values[te_ind]\n",
    "        X_tr_fold=scaler.fit_transform(X_tr_fold)\n",
    "        X_te_fold=scaler.transform(X_te_fold)\n",
    "\n",
    "        if c==\"logreg\":\n",
    "            clf=LogisticRegression(random_state=1)\n",
    "        elif c==\"rf\":\n",
    "            clf=RandomForestClassifier(random_state=1)\n",
    "        elif c==\"gsvm\":\n",
    "            clf = SVC(\n",
    "            kernel='rbf',random_state=123456)\n",
    "        elif c==\"knn\":\n",
    "            clf = KNeighborsClassifier()\n",
    "        elif c==\"svm\":\n",
    "            clf = SVC(kernel='linear' random_state=123456)\n",
    "        clf.fit(X_tr_fold,y_tr_fold)\n",
    "        #X_te_fold=fe.transform(X_te_fold)\n",
    "        y_te_pred=clf.predict(X_te_fold)\n",
    "        f1=f1_score(y_te_pred,y_te_fold, average='macro')\n",
    "        print(f1)\n",
    "        f1s+=f1 \n",
    "        accs+=accuracy_score(y_te_pred,y_te_fold)\n",
    "        conf_mat+=confusion_matrix(y_te_fold,y_te_pred, labels=labels)\n",
    "    np.savetxt(path+os.sep+\"conf_mat_folds_\"+settings+\"_\"+c+\".out\", conf_mat, delimiter=\",\")\n",
    "    file = open(path+os.sep+\"f1s_folds_\"+settings+\"_\"+c+\".out\",'w')  # w : writing mode  /  r : reading mode  /  a  :  appending mode\n",
    "    file.write('f1, {}\\n'.format(f1s/10))\n",
    "    file.write('acc, {}\\n'.format(accs/10))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def micro_tokenize(txt):\n",
    "    words = []\n",
    "    # split at whitespace\n",
    "    for w in txt.split():\n",
    "        w = w.strip('.,!?:;\"-+()„“”»«…\\'`’*')\n",
    "        # words need to contain at least one \"regular\" character\n",
    "        if re.search(r'[a-zöüßA-ZÄÖÜ]', w):\n",
    "            words.append(w)\n",
    "    return words\n",
    "\n",
    "def normalize(txt):\n",
    "    txt = txt.lower()\n",
    "\n",
    "    # replace URLs\n",
    "    url_re1 = re.compile(r'(?:ftp|http)s?://[\\w\\d:#@%/;$()~_?+=\\,.&#!|-]+')\n",
    "    txt = url_re1.sub('URL', txt)\n",
    "    url_re2 = re.compile(r'\\bwww\\.[a-zA-Z0-9-]{2,63}\\.[\\w\\d:#@%/;$()~_?+=\\,.&#!|-]+')\n",
    "    txt = url_re2.sub('URL', txt)\n",
    "    url_re3 = re.compile(r'\\b[a-zA-Z0-9.]+\\.(?:com|org|net|io)')\n",
    "    txt = url_re3.sub('URL', txt)\n",
    "    \n",
    "    # remove repeated symbols\n",
    "    for s in ',.!?:;#-_=+*/$@%<>&()[]':\n",
    "        txt = re.sub('[%s]+' % s, s, txt)\n",
    "\n",
    "    # separate punctuation\n",
    "    txt = re.sub(r'([.,!?:;/()\\'\"„“”»«`’…$%*])', r' \\1 ', txt)\n",
    "\n",
    "    # remove leading, trailing and repeated whitespace\n",
    "    txt = txt.strip()\n",
    "    txt = re.sub(r'\\s+', ' ', txt)\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = .1\n",
    "N_FOLDS=10\n",
    "path=\"results_with_scaling_default\"\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "y_labels=pd.read_csv(features_path+os.sep+\"y_labels.out\", index_col=0, header=None) \n",
    "settings=\"counts\"\n",
    "#sentences_embeddings = np.genfromtxt(\"Fasttext_embeddings.out\", delimiter=',')\n",
    "sentences_embeddings=tr_Df.Text\n",
    "kf=StratifiedKFold(n_splits=N_FOLDS)\n",
    "\n",
    "labels=y_labels.iloc[:,0].unique()\n",
    "for c in [\"gsvm\", \"knn\",\"svm\",\"logreg\", \"nb\",\"rf\"]:\n",
    "    kf=StratifiedKFold(n_splits=N_FOLDS)\n",
    "    conf_mat=np.zeros((9,9))\n",
    "    f1s=0\n",
    "    accs=0\n",
    "    scaler = MinMaxScaler()\n",
    "    for tr_ind, te_ind in kf.split(sentences_embeddings, y_labels):\n",
    "        X_tr_fold=sentences_embeddings[tr_ind]\n",
    "        X_te_fold=sentences_embeddings[te_ind]\n",
    "        y_tr_fold=y_labels.values[tr_ind]\n",
    "        y_te_fold=y_labels.values[te_ind]\n",
    "        #X_tr_fold=scaler.fit_transform(X_tr_fold)\n",
    "        #X_te_fold=scaler.transform(X_te_fold)\n",
    "        \n",
    "        fe = CountVectorizer(\n",
    "            preprocessor=normalize,\n",
    "            tokenizer=micro_tokenize,\n",
    "            binary=True,\n",
    "        )\n",
    "        fe.fit(X_tr_fold)\n",
    "        X_tr_fold = fe.transform(X_tr_fold)\n",
    "\n",
    "        if c==\"logreg\":\n",
    "            clf=LogisticRegression(random_state=1)\n",
    "        elif c==\"rf\":\n",
    "            clf=RandomForestClassifier(random_state=1)\n",
    "        elif c==\"svm\":\n",
    "            clf = SVC(\n",
    "            kernel='linear',random_state=123456)\n",
    "        elif c==\"gsvm\":\n",
    "            clf = SVC(\n",
    "            kernel='rbf',random_state=123456)\n",
    "        elif c==\"knn\":\n",
    "            clf = KNeighborsClassifier()\n",
    "        elif c==\"nb\":\n",
    "            clf=MultinomialNB()\n",
    "        clf.fit(X_tr_fold,y_tr_fold)\n",
    "        X_te_fold=fe.transform(X_te_fold)\n",
    "        y_te_pred=clf.predict(X_te_fold)\n",
    "        f1=f1_score(y_te_pred,y_te_fold, average='macro')\n",
    "        print(f1)\n",
    "        f1s+=f1 \n",
    "        accs+=accuracy_score(y_te_pred,y_te_fold)\n",
    "        conf_mat+=confusion_matrix(y_te_fold,y_te_pred, labels=labels)\n",
    "    np.savetxt(path+os.sep+\"conf_mat_folds_\"+settings+\"_\"+c+\".out\", conf_mat, delimiter=\",\")\n",
    "    file = open(path+os.sep+\"f1s_folds_\"+settings+\"_\"+c+\".out\",'w')  # w : writing mode  /  r : reading mode  /  a  :  appending mode\n",
    "    file.write('f1, {}\\n'.format(f1s/10))\n",
    "    file.write('acc, {}\\n'.format(accs/10))\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
